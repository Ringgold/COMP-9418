{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9418 - Assignment 1 - Bayesian Networks as Classifiers\n",
    "\n",
    "## UNSW Sydney, September 2021\n",
    "\n",
    "- Student name 1 - zID\n",
    "- Student name 2 - ZID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "**Submission deadline:** Sunday, 17th October 2021, at 18:00:00.\n",
    "\n",
    "**Late Submission Policy:** The penalty is set at 20% per late day. This is ceiling penalty, so if a group is marked 60/100 and they submitted two days late, they still get 60/100.\n",
    "\n",
    "**Form of Submission:** This is a group assignment. Each group can have up to **two** students. **Only one member of the group should submit the assignment**.\n",
    "\n",
    "You can reuse any piece of source code developed in the tutorials.\n",
    "\n",
    "Submit your files using give. On a CSE Linux machine, type the following on the command-line:\n",
    "\n",
    "``$ give cs9418 ass1 solution.zip``\n",
    "\n",
    "Alternative, you can submit your solution via [WebCMS](https://webcms3.cse.unsw.edu.au/COMP9418/21T3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical prerequisites\n",
    "\n",
    "These are the libraries your are allowed to use. No other libraries will be accepted. Make sure you are using Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allowed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import heapq as pq\n",
    "import matplotlib as mp\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "from graphviz import Digraph\n",
    "from tabulate import tabulate\n",
    "import copy,time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the python files we developed in tutorials, or any other code from the tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiscreteFactors import Factor\n",
    "from Graph import Graph\n",
    "from BayesNet import BayesNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial task - Initialise graph\n",
    "\n",
    "Create a graph ``G`` that represents the following network by filling in the edge lists.\n",
    "![Bayes Net](BayesNet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Graph({\n",
    "    \"BreastDensity\" : [\"Mass\"],\n",
    "    \"Location\" : [\"BC\"],\n",
    "    \"Age\" : [\"BC\"],\n",
    "    \"BC\" : [\"Metastasis\", \"MC\", \"SkinRetract\", \"NippleDischarge\", \"AD\", \"Mass\"],\n",
    "    \"Mass\" : [\"Size\",  \"Shape\", \"Margin\"],\n",
    "    \"AD\" : [\"FibrTissueDev\"],\n",
    "    \"Metastasis\" : [\"LymphNodes\"],\n",
    "    \"MC\" : [],\n",
    "    \"Size\" : [],\n",
    "    \"Shape\" : [],\n",
    "    \"FibrTissueDev\" : [\"SkinRetract\" , \"NippleDischarge\",\"Spiculation\"],\n",
    "    \"LymphNodes\" : [],\n",
    "    \"SkinRetract\" : [],\n",
    "    \"NippleDischarge\" : [],\n",
    "    \"Spiculation\" : [\"Margin\"],\n",
    "    \"Margin\" : [],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('bc.csv') as file:\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "#remove 2 variables from data (because we are pretending we don't know this information)\n",
    "if 'Metastasis' in data:\n",
    "    del data['Metastasis']\n",
    "if 'LymphNodes' in data:\n",
    "    del data['LymphNodes']\n",
    "\n",
    "# remove same 2 nodes from graph\n",
    "G.remove_node('Metastasis')\n",
    "G.remove_node('LymphNodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 1 - Efficient d-separation test\n",
    "\n",
    "Implement the efficient version of the d-separation algorithm in a function ``d_separation(G, X, Z, Y)`` that return a boolean: ``True`` if **X** is d-separated from **Y** given **Z** in the graph $G$ and ``False`` otherwise.\n",
    "\n",
    "* **X**,**Y** and **Z** are python sets, each containing a set of variable names. \n",
    "* Variable names may be strings or integers, and can be assumed to be nodes of the graph $G$. \n",
    "* $G$ is a directed graph object as defined in tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for d_separation(G, X, Z, Y) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all leaves not in the XYZ\n",
    "def del_leaf(G,node_set):\n",
    "    flag = True\n",
    "    while flag:\n",
    "        flag = False\n",
    "        for node in list(node_set):\n",
    "            if not G.adj_list[node]:\n",
    "                G.remove_node(node)\n",
    "                node_set.remove(node)\n",
    "                flag = True\n",
    "    G.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_separation(G, X, Z, Y):\n",
    "    '''\n",
    "    Arguments:\n",
    "    G:   is an object of type Graph (the class you developed in tutorial 1)\n",
    "    X,Z and Y:  are python `set` objects.\n",
    "    '''\n",
    "    ans = True\n",
    "    xyzset = X.union(Y).union(Z)\n",
    "    nodeset = set(G.adj_list.keys())\n",
    "    \n",
    "    not_in_xyzset = set(nodeset  - xyzset)\n",
    "    G1 = copy.deepcopy(G)\n",
    "    del_leaf(G1,not_in_xyzset)\n",
    "    for node in Z:\n",
    "        G1.adj_list[node] = []\n",
    "    for k,v in G1.adj_list.items():\n",
    "        if bool(v):\n",
    "            for node in v:\n",
    "                G1.adj_list[node].append(k)\n",
    "                \n",
    "    for node in X:\n",
    "        G1.dfs(node)\n",
    "        Ycolour = [G1.colour[node] for node in Y]\n",
    "        if 'black' in Ycolour:\n",
    "            ans = False\n",
    "    #G1.show()\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "## Note: More hidden tests will be used. You should make more tests yourself.\n",
    "\n",
    "def test(statement):\n",
    "    if statement:\n",
    "        print(\"Passed test case\")\n",
    "    else:\n",
    "        print(\"Failed test case\")\n",
    "        \n",
    "test(d_separation(G, set(['Age']), set(['BC']), set(['AD'])))\n",
    "test(not d_separation(G, set(['Spiculation','SkinRetract']), set(['MC', 'Size']), set(['Age'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Marks] Task 2 - Estimate Bayesian Network parameters from data\n",
    "\n",
    "Implement a function ``learn_outcome_space(data)`` that learns the outcome space (the valid values for each variable) from the pandas dataframe ``data`` and returns a dictionary ``outcomeSpace`` with these values.\n",
    "\n",
    "Implement a method ``model.learn_parameters(data, alpha=1)`` that learns the parameters of the Bayesian Network `model`. This function should do the same as the ``learn_parameters`` function from tutorials, but it should also implement laplacian smoothing with parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_outcome_space(data) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_outcome_space(data):\n",
    "    '''\n",
    "    Arguments:\n",
    "        data - A pandas dataframe\n",
    "    Returns: \n",
    "        outcomeSpace - A dictionary. e.g. {'A':('True', 'False'), 'B':('up','down','left'), 'C':(1,2,3,4)}\n",
    "    '''\n",
    "    outcomeSpace = {}\n",
    "    for col in data.columns:\n",
    "        outcomeSpace[col] = tuple(data[col].unique())\n",
    "    return outcomeSpace\n",
    "\n",
    "\n",
    "outcomeSpace = learn_outcome_space(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "outcomeSpace = learn_outcome_space(data)\n",
    "\n",
    "outcomes = outcomeSpace['BreastDensity']\n",
    "answer = ('high', 'medium', 'low')\n",
    "test(len(outcomes) == len(answer) and set(outcomes) == set(answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learnParameters in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allEqualThisIndex(dict_of_arrays, **fixed_vars):\n",
    "    # base index is a boolean vector, everywhere true\n",
    "    first_array = dict_of_arrays[list(dict_of_arrays.keys())[0]]\n",
    "    index = np.ones_like(first_array, dtype=np.bool_)\n",
    "    for var_name, var_val in fixed_vars.items():\n",
    "        index = index & (np.asarray(dict_of_arrays[var_name])==var_val)\n",
    "    return index\n",
    "\n",
    "def estimateFactor(data, var_name, parent_names, outcomeSpace,alpha):\n",
    "    var_outcomes = outcomeSpace[var_name]\n",
    "    parent_outcomes = [outcomeSpace[var] for var in (parent_names)]\n",
    "    # cartesian product to generate a table of all possible outcomes\n",
    "    all_parent_combinations = product(*parent_outcomes)\n",
    "\n",
    "    f = Factor(list(parent_names)+[var_name], outcomeSpace)\n",
    "    \n",
    "    for i, parent_combination in enumerate(all_parent_combinations):\n",
    "        parent_vars = dict(zip(parent_names, parent_combination))\n",
    "        parent_index = allEqualThisIndex(data, **parent_vars)\n",
    "        for var_outcome in var_outcomes:\n",
    "            var_index = (np.asarray(data[var_name])==var_outcome)\n",
    "            f[tuple(list(parent_combination)+[var_outcome])] = ((var_index & parent_index).sum()+alpha)/(parent_index.sum()+alpha*len(var_outcomes))\n",
    "            \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(BayesNet):\n",
    "    def learn_parameters(self, data, alpha=1):\n",
    "        graphT = self.graph.transpose()\n",
    "        for node, parents in graphT.adj_list.items():\n",
    "            f = estimateFactor(data, node, parents, self.outcomeSpace,alpha)\n",
    "            self.factors[node] = f\n",
    "\n",
    "model = BayesNet(G,outcomeSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "model.learn_parameters(data, alpha=1)\n",
    "\n",
    "test(model.factors['Age']['35-49'] == 0.248000399920016)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 3 - Bayesian Network Classification\n",
    "\n",
    "Design a new function ``assess_bayes_net(model, dataframe, var)`` that uses the test cases in ``dataframe`` to assess the performance of the Bayesian network at classifying the variable `var`. Implement the efficient classification procedure discussed in the lectures (make sure you understand what a Markov Blanket is). Such a function should return the classifier accuracy. \n",
    "\n",
    " * ``var`` is the name of the variable you are predicting, using the values of all the other variables. \n",
    " \n",
    "If you like, you can add new functions to the BayesNet class, or write helper functions to help solve the above task.\n",
    "\n",
    "Using another function called `cross_validation_bayes_net`, compute and report the average accuracy over the ten cross-validation runs as well as the standard deviation. A scaffold for this function is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for assess_bayes_net in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here, we create our own query function so that:\n",
    "1. We do not need to join (all of) the variables' prop table every single time when doing query\n",
    "2. We can pass custimized table for query, and prevent using oversized table for small queries\n",
    "'''\n",
    "\n",
    "\n",
    "class BayesNet(BayesNet):\n",
    "    def query_without_self_joint(self, table, q_vars, **q_evi):\n",
    "        \"\"\"\n",
    "        arguments\n",
    "        `table`, the table used for query\n",
    "        `q_vars`, list of variables in query head\n",
    "        `q_evi`, dictionary of evidence in the form of variables names and values\n",
    "\n",
    "        Returns a new NORMALIZED factor will all hidden variables eliminated as evidence set as in q_evi\n",
    "        \"\"\"\n",
    "        assert isinstance(q_vars, list) or isinstance(q_vars, tuple), \"q_vars should be a list\"\n",
    "\n",
    "        f = table\n",
    "\n",
    "        # First, we set the evidence\n",
    "        f = f.evidence(**q_evi)\n",
    "\n",
    "        # Second, we eliminate hidden variables NOT in the query\n",
    "        for var in self.outcomeSpace:\n",
    "            if var not in q_vars:\n",
    "                f = f.marginalize(var)\n",
    "        return f.normalize()\n",
    "\n",
    "\n",
    "def get_parents(G):\n",
    "    '''\n",
    "    Create a parent nodes dict list for input graph and return it\n",
    "    '''\n",
    "    graph_parents_list = {}\n",
    "    for node in G:\n",
    "        graph_parents_list[node] = []\n",
    "    for parent_node in G:\n",
    "        for child_node in G[parent_node]:\n",
    "            graph_parents_list[child_node].append(parent_node)\n",
    "    return graph_parents_list\n",
    "\n",
    "\n",
    "def get_markov_blanket(model: BayesNet, var: str):\n",
    "    '''\n",
    "    1. Find all parents\n",
    "    2. Find all children\n",
    "    3. Find all spouses\n",
    "    '''\n",
    "    markov_blanket = []\n",
    "    graph = model.graph.adj_list\n",
    "    # add children\n",
    "    markov_blanket = markov_blanket + graph[var]\n",
    "    # add parent\n",
    "    graph_parents_list = get_parents(graph)\n",
    "    markov_blanket = markov_blanket + graph_parents_list[var]\n",
    "\n",
    "    # add all spouses\n",
    "    for node in graph[var]:\n",
    "        markov_blanket = markov_blanket + graph_parents_list[node]\n",
    "    markov_blanket = list(dict.fromkeys(markov_blanket))\n",
    "\n",
    "    # Make sure both verions of markov blankets are returned\n",
    "    markov_blanket_without_var = copy.deepcopy(markov_blanket)\n",
    "    if var in markov_blanket:\n",
    "        markov_blanket_without_var.remove(var)\n",
    "    else:\n",
    "        markov_blanket.append(var)\n",
    "\n",
    "    return markov_blanket, markov_blanket_without_var\n",
    "\n",
    "\n",
    "def get_related_table(model: BayesNet, var_space: list):\n",
    "    '''\n",
    "    Get the joint table of a given bayes net model of a given variable space\n",
    "    '''\n",
    "    all_table = model.factors\n",
    "    table = all_table[var_space[0]]\n",
    "    for i in range(1, len(var_space)):\n",
    "        table = table.join(all_table[var_space[i]])\n",
    "    return table\n",
    "\n",
    "\n",
    "def assess_bayes_net(model, dataframe, var='BC'):\n",
    "    '''\n",
    "    Test the accuracy given trained model by using the test dataframe\n",
    "    '''\n",
    "    # Get the probability table of all the variables in 'var' markov blanket\n",
    "    markov_blanket, markov_blanket_without_var = get_markov_blanket(model, var)\n",
    "    related_test_table = dataframe[markov_blanket]\n",
    "    correct_amount = 0\n",
    "    total_amount = 0\n",
    "    joint_table = get_related_table(model, markov_blanket)\n",
    "    for row_index, row_data in related_test_table.iterrows():\n",
    "        # Get evidence and actual value of var from the test data\n",
    "        var_value_actual = row_data.pop(var)\n",
    "        row_data_dict = dict(row_data)\n",
    "\n",
    "        # get p(var|evidence) in an array\n",
    "        query_result = model.query_without_self_joint(joint_table, [var], **row_data_dict)\n",
    "        var_possible_values = model.outcomeSpace[var]\n",
    "\n",
    "        # Get predicted value according to the query table result\n",
    "        possibility_max = 0\n",
    "        var_value_prediced = None\n",
    "        for value in var_possible_values:\n",
    "            possibility = query_result[value]\n",
    "            if possibility > possibility_max:\n",
    "                possibility_max = possibility\n",
    "                var_value_prediced = value\n",
    "\n",
    "        # Compare predicted value and actual value of the variable with current evidence\n",
    "        if var_value_prediced == var_value_actual:\n",
    "            correct_amount += 1\n",
    "        total_amount += 1\n",
    "    accuracy = correct_amount / total_amount\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def cross_validation_bayes_net(dataframe, var='BC', k=10):\n",
    "    accuracy_list = []\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    model = BayesNet(G, outcomeSpace)\n",
    "\n",
    "    for i in range(k):\n",
    "        # split dataset into train and test\n",
    "        length = int(len(dataframe) / k)\n",
    "        data_train = data.drop([e for e in range(i * length, (i + 1) * length)])\n",
    "        data_test = data.loc[i * length:(i + 1) * length - 1]\n",
    "        # train a model\n",
    "        model.learn_parameters(data_train)\n",
    "\n",
    "        # test the model with assess_bayes_net\n",
    "        acc = assess_bayes_net(model, data_test, var)\n",
    "        print(\"Acc of valid \" + str(i) + \" is: \" + str(acc))\n",
    "        accuracy_list.append(acc)\n",
    "    print(\"mean and std: \", np.mean(accuracy_list), np.std(accuracy_list))\n",
    "    return np.mean(accuracy_list), np.std(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc of valid 0 is: 0.841\n",
      "Acc of valid 1 is: 0.8445\n",
      "Acc of valid 2 is: 0.836\n",
      "Acc of valid 3 is: 0.848\n",
      "Acc of valid 4 is: 0.846\n",
      "Acc of valid 5 is: 0.843\n",
      "Acc of valid 6 is: 0.8285\n",
      "Acc of valid 7 is: 0.84\n",
      "Acc of valid 8 is: 0.837\n",
      "Acc of valid 9 is: 0.848\n",
      "mean and std:  0.8412000000000001 0.00580603134679791\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "acc, stddev = cross_validation_bayes_net(data, 'BC', 10)\n",
    "test(abs(acc - 0.85) < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Marks] Task 4 - Naïve Bayes Classification\n",
    "\n",
    "Design a new function ``assess_naive_bayes(model, data, var)`` to classify and assess the test cases in ``data``. To classify each example, use the log probability trick discussed in the lectures. Do $k$-fold cross-validation with the `cross_validation_naive_bayes(data, var, k)` function, same as above, and return ``acc`` and ``stddev``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for assess_naive_bayes(model, data, var) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_naive_bayes(model, dataframe, var='BC'):\n",
    "    correct = 0\n",
    "\n",
    "    for index, single_data in dataframe.iterrows():\n",
    "        plist = dict()\n",
    "        predict = None\n",
    "        for outcome in model.outcomeSpace[var]:\n",
    "            # prior\n",
    "            p_outcome = np.log(model.factors[var][(outcome)])\n",
    "\n",
    "            for node in model.graph.adj_list[var]:\n",
    "\n",
    "                #p += log(p(node|var))\n",
    "                p_outcome += np.log(model.factors[node][(outcome,single_data[node])])\n",
    "\n",
    "            # keep all the p\n",
    "            plist[outcome] = p_outcome\n",
    "        #take the largest p as predict\n",
    "        predict = list(plist.keys())[list(plist.values()).index(max(plist.values()))]\n",
    "\n",
    "        if predict == single_data[var]:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(dataframe)\n",
    "    return accuracy\n",
    "\n",
    "def cross_validation_naive_bayes(dataframe, var='BC', k=10):\n",
    "    accuracy_list = []\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    g_nb = copy.deepcopy(G)\n",
    "    # process the graph to make it a naive bayes graph (var to all other nodes)\n",
    "    for node in g_nb.adj_list.keys():\n",
    "        g_nb.adj_list[node] = []\n",
    "    g_nb.adj_list[var] = list(g_nb.adj_list.keys())\n",
    "    g_nb.adj_list[var].remove(var)\n",
    "    for i in range(k):\n",
    "        # split dataset into train and test\n",
    "\n",
    "        length = int(len(dataframe) / k)\n",
    "        data_train = data.drop([e for e in range(i * length, (i + 1) * length)])\n",
    "        data_test = data.loc[i * length:(i + 1) * length - 1]\n",
    "\n",
    "        # create and train a model\n",
    "\n",
    "        nbmodel = BayesNet(g_nb, outcomeSpace)\n",
    "        nbmodel.learn_parameters(data_train)\n",
    "\n",
    "        # test the model with assess_naive_bayes\n",
    "        acc = assess_naive_bayes(nbmodel, data_test)\n",
    "\n",
    "        accuracy_list.append(acc)\n",
    "    return np.mean(accuracy_list), np.std(accuracy_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "acc, stddev = cross_validation_naive_bayes(data, 'BC')\n",
    "test(abs(acc - 0.80) < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 5 - Tree-augmented Naïve Bayes Classification\n",
    "\n",
    "Similarly to the previous task, implement a Tree-augmented Naïve Bayes (TAN) classifier and evaluate your implementation in the breast cancer dataset. Design a function ``learn_tan_structure(data, class_var)`` to learn the TAN structure (graph) from ``data`` and return such a structure. Scaffolds for required functions are given below. Implement other helper functions as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_tan_structure(data) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I(X;Y|Z)=H(X|Z)+H(Y|Z)-H(X,Y|Z) from wikipedia of Conditional mutual information\n",
    "# H(Y|X)=-sigma_x,y p(x,y)*log(p(x,y)/p(x))\n",
    "# then we have I(i;j|C)=H(i|C)+H(j|C)-H(i,j|C)\n",
    "# H(i|C)= -sigma_i,C p(C,i)*log(p(C,i)/p(C))\n",
    "\n",
    "def get_mi(Ai,Aj,class_var,data):\n",
    "    #get p(c)\n",
    "    num_c = dict(data[class_var].value_counts(sort=False))\n",
    "    p_c = {key: value / len(data) for key, value in num_c.items()}\n",
    "\n",
    "    #get p(Ai,c)\n",
    "    varlist_i = [Ai,class_var]\n",
    "    num_ic = dict(data.groupby(varlist_i).size())\n",
    "    p_ic = {key: value / len(data) for key, value in num_ic.items()}\n",
    "    entropyic = 0\n",
    "    #get H(Ai|c)\n",
    "    for key, value in p_ic.items():\n",
    "        # key is (outcome of v1, outcome of c)\n",
    "        # so key[-1] is outcome of c\n",
    "        entropyic -= value * np.log(value / p_c[key[-1]])\n",
    "\n",
    "    #get p(Aj,c)\n",
    "    varlist_j = [Aj, class_var]\n",
    "    num_jc = dict(data.groupby(varlist_j).size())\n",
    "    p_jc = {key: value / len(data) for key, value in num_jc.items()}\n",
    "    entropyjc = 0\n",
    "    #get H(Aj|c)\n",
    "    for key, value in p_jc.items():\n",
    "        # key is (outcome of v2, outcome of c)\n",
    "        # so key[-1] is outcome of c\n",
    "        entropyjc -= value * np.log(value / p_c[key[-1]])\n",
    "\n",
    "    #get p(Ai,Aj,c)\n",
    "    varlist_ij = [Ai, Aj, class_var]\n",
    "    num_ijc = dict(data.groupby(varlist_ij).size())\n",
    "    p_ijc = {key: value / len(data) for key, value in num_ijc.items()}\n",
    "    entropyijc = 0\n",
    "    #get H(Ai,Aj|c)\n",
    "    for key, value in p_ijc.items():\n",
    "        # key is (outcome of v1, outcome of v2, outcome of c)\n",
    "        # so key[-1] is outcome of c\n",
    "        entropyijc -= value * np.log(value / p_c[key[-1]])\n",
    "\n",
    "    conditinal_mi = entropyic + entropyjc - entropyijc\n",
    "    return conditinal_mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(Graph):\n",
    "    def max_spanning_tree(self, start):\n",
    "        # modify the prim function is Graph.py\n",
    "        # make the weight negative to turn it to a max-heap\n",
    "        \"\"\"\n",
    "                argument\n",
    "                `start`, start vertex\n",
    "                \"\"\"\n",
    "        visited = {start}\n",
    "        Q = []\n",
    "        tree = Graph()\n",
    "        for e in self.adj_list[start]:\n",
    "            pq.heappush(Q, (-self.edge_weights[(start, e)], start, e))\n",
    "        while len(Q) > 0:\n",
    "            weight, v, u = pq.heappop(Q)\n",
    "            weight = -weight\n",
    "            if u not in visited:\n",
    "                visited.add(u)\n",
    "                tree.add_edge(v, u, weight=weight)\n",
    "                for e in self.adj_list[u]:\n",
    "                    if e not in visited:\n",
    "                        pq.heappush(Q, (-self.edge_weights[(u, e)], u, e))\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_tan_structure(data, class_var='BC'):\n",
    "    '''\n",
    "    Arguments:\n",
    "        data: a dataframe\n",
    "        class_var: The variable you will be classifying with this graph structure\n",
    "    Return:\n",
    "        graph: A Graph object\n",
    "    '''\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    node_list = list(outcomeSpace.keys())\n",
    "\n",
    "    g = Graph({node:[] for node in node_list})\n",
    "    node_list.remove(class_var)\n",
    "\n",
    "    for i in range(len(node_list)-1):\n",
    "        for j in range(i+1,len(node_list)):\n",
    "            Ai = node_list[i]\n",
    "            Aj = node_list[j]\n",
    "            conditional_mi = get_mi(Ai,Aj,class_var,data)\n",
    "            g.add_edge(Ai,Aj,conditional_mi,directed=False)\n",
    "    # no gt since the max_spanning_tree algorithm add directions\n",
    "    # undirected edge is represented by two directed edges between two nodes\n",
    "    # when it picks one edge it will discard another to avoid loop\n",
    "    # random start point -- does not affect the outcome of MST\n",
    "    gtd = g.max_spanning_tree(node_list[0])\n",
    "    gtd.add_node(class_var)\n",
    "    gtd.adj_list[class_var] = node_list\n",
    "    return gtd\n",
    "\n",
    "def cross_validation_tan(data, var='BC', k=10):\n",
    "    accuracy_list = []\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    for i in range(k):\n",
    "        # split dataset into train and test\n",
    "\n",
    "        length = int(len(data) / k)\n",
    "        data_train = data.drop([e for e in range(i * length, (i + 1) * length)])\n",
    "        data_test = data.loc[i * length:(i + 1) * length - 1]\n",
    "\n",
    "        # create and train a model\n",
    "\n",
    "        gtd = learn_tan_structure(data_train, var)\n",
    "        model = BayesNet(gtd, outcomeSpace)\n",
    "        model.learn_parameters(data_train)\n",
    "\n",
    "        # test the model with assess_naive_bayes\n",
    "        acc = assess_bayes_net(model, data_test)\n",
    "\n",
    "        accuracy_list.append(acc)\n",
    "    return np.mean(accuracy_list), np.std(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 5 tan structure\n",
      "Passed test case\n",
      "Passed test case\n",
      "Passed test case\n",
      "task 5 cv tan\n",
      "Passed test case\n",
      "Passed test case\n",
      "Passed test case\n",
      "0.83295\n",
      "Passed test case\n",
      "Passed test case\n",
      "time of cv_tan and std:  42.29635572433472 0.006262786919575036\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "print(\"task 5 tan structure\")\n",
    "tan_graph = learn_tan_structure(data)\n",
    "test(len(tan_graph.children('BC')) == len(tan_graph)-1)\n",
    "test('FibrTissueDev' in tan_graph.children('Spiculation') or 'Spiculation' in tan_graph.children('FibrTissueDev'))\n",
    "\n",
    "sum_edges = 0\n",
    "for node, children in tan_graph.adj_list.items():\n",
    "    if node != 'BC':\n",
    "        sum_edges += len(children)\n",
    "test(sum_edges == len(tan_graph)-2) #number of edges counted in the tree is num nodes in tree - 1 (which is nodes in graph-2 since BC doesn't count)\n",
    "\n",
    "\n",
    "\n",
    "print(\"task 5 cv tan\")\n",
    "s = time.time()\n",
    "acc, stddev = cross_validation_tan(data, 'BC')\n",
    "e = time.time()\n",
    "test(e-s < 50)\n",
    "test(e-s < 100)\n",
    "test(e-s < 300)\n",
    "print(acc)\n",
    "test(abs(acc - 0.84195) < 0.01)\n",
    "test(stddev > 0 and stddev < 0.01) # changed this because several students whose code looks correct weren't passing\n",
    "\n",
    "print(\"time of cv_tan and std: \",e-s,stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 6 - Report\n",
    "\n",
    "Write a report (**with less than 500 words**) summarising your findings in this assignment. Your report should address the following:\n",
    "\n",
    "a. Make a summary and discussion of the experimental results. You can analyse your results from different aspects such as accuracy, runtime, coding complexity and independence assumptions. You can use plots to illustrate your results.\n",
    "\n",
    "b. Discuss the time and memory complexity of the implemented algorithms.\n",
    "\n",
    "Use Markdown and Latex to write your report in the Jupyter notebook. Develop some plots using Matplotlib to illustrate your results. Be mindful of the maximum number of words. Please, be concise and objective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
